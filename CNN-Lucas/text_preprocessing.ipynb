{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Duplicate registrations for type 'experimentalOptimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# import seaborn as sns\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# import tensorflow_text as text\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/__init__.py:473\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(_current_module, \u001b[39m\"\u001b[39m\u001b[39mkeras\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    472\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 473\u001b[0m     keras\u001b[39m.\u001b[39;49m_load()\n\u001b[1;32m    474\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m    475\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/lazy_loader.py:41\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_module_globals[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_name] \u001b[39m=\u001b[39m module\n\u001b[1;32m     44\u001b[0m \u001b[39m# Emit a warning if one was specified\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/keras/__init__.py:25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[1;32m     24\u001b[0m \u001b[39m# See b/110718070#comment18 for more details about this import.\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Input\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/keras/models/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m Functional\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/functional.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m base_layer\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m base_layer_utils\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m functional_utils\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/base_layer.py:43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m node \u001b[39mas\u001b[39;00m node_module\n\u001b[1;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmixed_precision\u001b[39;00m \u001b[39mimport\u001b[39;00m autocast_variable\n\u001b[0;32m---> 43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmixed_precision\u001b[39;00m \u001b[39mimport\u001b[39;00m loss_scale_optimizer\n\u001b[1;32m     44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmixed_precision\u001b[39;00m \u001b[39mimport\u001b[39;00m policy\n\u001b[1;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaving\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msaved_model\u001b[39;00m \u001b[39mimport\u001b[39;00m layer_serialization\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/keras/mixed_precision/loss_scale_optimizer.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m optimizers\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmixed_precision\u001b[39;00m \u001b[39mimport\u001b[39;00m loss_scale \u001b[39mas\u001b[39;00m keras_loss_scale_module\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizer_experimental\u001b[39;00m \u001b[39mimport\u001b[39;00m optimizer \u001b[39mas\u001b[39;00m optimizer_experimental\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizer_v2\u001b[39;00m \u001b[39mimport\u001b[39;00m optimizer_v2\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizer_v2\u001b[39;00m \u001b[39mimport\u001b[39;00m utils \u001b[39mas\u001b[39;00m optimizer_utils\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizer_experimental/optimizer.py:649\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    643\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRestoring functional Optimizers from SavedModels is not currently \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    644\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msupported. Please file a feature request if this limitation bothers \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    645\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    648\u001b[0m \u001b[39m# Register the optimizer for loading from saved_model purpose.\u001b[39;00m\n\u001b[0;32m--> 649\u001b[0m tf\u001b[39m.\u001b[39;49m__internal__\u001b[39m.\u001b[39;49msaved_model\u001b[39m.\u001b[39;49mload\u001b[39m.\u001b[39;49mregister_revived_type(\n\u001b[1;32m    650\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mexperimentalOptimizer\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    651\u001b[0m     \u001b[39mlambda\u001b[39;49;00m obj: \u001b[39misinstance\u001b[39;49m(obj, Optimizer),\n\u001b[1;32m    652\u001b[0m     versions\u001b[39m=\u001b[39;49m[\n\u001b[1;32m    653\u001b[0m         tf\u001b[39m.\u001b[39;49m__internal__\u001b[39m.\u001b[39;49msaved_model\u001b[39m.\u001b[39;49mload\u001b[39m.\u001b[39;49mVersionedTypeRegistration(\n\u001b[1;32m    654\u001b[0m             object_factory\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m proto: RestoredOptimizer(),\n\u001b[1;32m    655\u001b[0m             version\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m    656\u001b[0m             min_producer_version\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    657\u001b[0m             min_consumer_version\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    658\u001b[0m     ])\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/saved_model/revived_types.py:133\u001b[0m, in \u001b[0;36mregister_revived_type\u001b[0;34m(identifier, predicate, versions)\u001b[0m\n\u001b[1;32m    130\u001b[0m   version_numbers\u001b[39m.\u001b[39madd(registration\u001b[39m.\u001b[39mversion)\n\u001b[1;32m    132\u001b[0m \u001b[39mif\u001b[39;00m identifier \u001b[39min\u001b[39;00m _REVIVED_TYPE_REGISTRY:\n\u001b[0;32m--> 133\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDuplicate registrations for type \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00midentifier\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m _REVIVED_TYPE_REGISTRY[identifier] \u001b[39m=\u001b[39m (predicate, versions)\n\u001b[1;32m    136\u001b[0m _TYPE_IDENTIFIERS\u001b[39m.\u001b[39mappend(identifier)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Duplicate registrations for type 'experimentalOptimizer'"
     ]
    }
   ],
   "source": [
    "#Necessary imports for our model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "# import tensorflow_text as text\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "# for Colab users: !pip install tensorflow_text\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_dict = Counter(stop_words)\n",
    "\n",
    "#load the data\n",
    "df = pd.read_csv('/Users/lucasion/Documents/GitHub/seng544-proj-2/CNN-Lucas/Data/tweets.csv')\n",
    "\n",
    "##########################\n",
    "### Text Preprocessing ###\n",
    "##########################\n",
    "\n",
    "#remove rt from begining of sentence - do first cause RT is capitalized.\n",
    "df[\"text\"] = df[\"text\"].map(lambda name: re.sub('^(RT)', ' ', name))\n",
    "#removing links\n",
    "df[\"text\"] = df[\"text\"].map(lambda name: re.sub(r'http\\S+', ' ', name))\n",
    "#removing mentions\n",
    "df[\"text\"] = df[\"text\"].map(lambda name: re.sub(\"@([a-zA-Z0-9_]{1,50})\", '', name))\n",
    "\n",
    "#remove repeated instances of characters\n",
    "#removing repeating characters\n",
    "repeat_pattern = re.compile(r'(\\w)\\1*') #compile the pattern we are looking for\n",
    "match_substitution = r'\\1' #substituion pattern\n",
    "df[\"text\"] = df[\"text\"].map(lambda name: re.sub(repeat_pattern, match_substitution, name))\n",
    "#removal of digits with regex - we do this here because it is possible to have numbers in tags and urls replace with space.\n",
    "df[\"text\"] = df[\"text\"].map(lambda name: re.sub(r'[0-9]', ' ', name))\n",
    "EMOJI_PATTERN = re.compile(\n",
    "    \"[\"\n",
    "    \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "    \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "    \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "    \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "    \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "    \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "    \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "    \"\\U000024C2-\\U0001F251\" \n",
    "    \"]+\")\n",
    "df[\"text\"] = df[\"text\"].map(lambda name: re.sub(EMOJI_PATTERN, ' ', name))\n",
    "#do this after removing mentions -> don't # here. ->replace with space.\n",
    "df[\"text\"] = df[\"text\"].map(lambda name: name.lower())\n",
    "special_pattern = re.compile('[!\\.\\^\\$\\|\\?\\*\\+\\=\\(\\)\\{\\}\\@\\=\\/\\<\\>\\,\\~\\`\\-\\%\\&\\:\\;\\[\\]\"“”…]')\n",
    "df[\"text\"] = df[\"text\"].map(lambda name: re.sub(special_pattern, ' ', name))\n",
    "#remove a hashtag if it has no significance, ie, not part of a #word\n",
    "df[\"text\"] = df[\"text\"].map(lambda name: re.sub('(#[^(a-zA-Z0-9)])', ' ', name))\n",
    "#removing doublicate spaces and all white spaces like \\t, \\n or \\r\n",
    "df[\"text\"] = df[\"text\"].map(lambda name: \" \".join(name.split()))\n",
    "#Now remove stop words\n",
    "df[\"text\"] = df[\"text\"].map(lambda name: ' '.join([word for word in name.split() if word not in stopwords_dict]))\n",
    "#After removing stop words we can clean up more\n",
    "df[\"text\"] = df[\"text\"].map(lambda name: re.sub('[\\']', ' ', name))\n",
    "#final white space clean up\n",
    "df[\"text\"] = df[\"text\"].map(lambda name: \" \".join(name.split(' ')))\n",
    "#still need to check for strings that contain whitespaces only and remove them\n",
    "df[\"text\"] = df[\"text\"].map(lambda text: np.nan if len(text) == 0 else text)\n",
    "df.dropna(axis=0, inplace=True)\n",
    "df.to_csv(\"testing.csv\", sep='\\t', encoding='utf-8')\n",
    "\n",
    "##############################\n",
    "### End Text Preprocessing ###\n",
    "##############################\n",
    "\n",
    "#Select number of countries that we want our model to examine (Top in # of tweets)\n",
    "num_of_top_countries = 20\n",
    "df = df[df[\"country\"].isin(df[\"country\"].value_counts()[:num_of_top_countries].index.values)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57f7f42ddb9d0ace86a28df5882cdbf244b73004f7298217daed66a072a973ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
